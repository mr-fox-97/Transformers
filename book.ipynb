{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multihead attention\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "Sean:\n",
    "\n",
    "- $Q \\in \\mathbb{R}^{s \\times d_q}$\n",
    "- $K \\in \\mathbb{R}^{s \\times d_k}$\n",
    "- $V \\in \\mathbb{R}^{s \\times d_v}$\n",
    "\n",
    "Con $s$ la longitud de la secuencia de tokens.\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = W^O \\text{Concat}(\\text{head}_1, \\text{head}_2, \\cdots, \\text{head}_h) $$\n",
    "$$ \\text{head}_i = \\text{Attention}(W^Q_iQ, W^K_iK, W^V_iV) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $X \\in \\mathbb{R}^{s \\times d}$ una secuencia de tokens.\n",
    "\n",
    "$$ X := \\begin{bmatrix} \n",
    "\n",
    "    x^1_1 & x^1_2 & \\cdots & x^1_d  \\\\\n",
    "    x^2_1 & x^2_2 & \\cdots & x^2_d   \\\\\n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "    x^s_1 & x^s_2 & \\cdots & x^s_d  \\\\\n",
    "\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "\n",
    "$$ X := \\begin{bmatrix} \n",
    "\n",
    "    \\begin{bmatrix} \n",
    "\n",
    "        x^1_1 & x^1_2 & \\cdots & x^1_{d/h}  \\\\\n",
    "\n",
    "        \n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "\n",
    "\n",
    "        x^1_{d\\frac{(h-1)}{h}+1} & x^1_2 & \\cdots & x^1_d  \\\\\n",
    "\n",
    "    \\end{bmatrix} \\\\\n",
    "\n",
    "    \\vdots \\\\\n",
    "\n",
    "    \n",
    "    \\begin{bmatrix} \n",
    "\n",
    "        x^s_1 & x^s_2 & \\cdots & x^s_{d/h}  \\\\\n",
    "\n",
    "        \n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "\n",
    "\n",
    "        x^s_{d\\frac{(h-1)}{h}+1} & x^s_2 & \\cdots & x^s_d  \\\\\n",
    "\n",
    "    \\end{bmatrix} \\\\\n",
    "\n",
    "\\end{bmatrix} $$ \n",
    "\n",
    "\n",
    "\n",
    "$$ X := \\begin{bmatrix} \n",
    "\n",
    "    \\begin{bmatrix} \n",
    "\n",
    "        x^1_1 & x^1_2 & \\cdots & x^1_{d/h}  \\\\\n",
    "\n",
    "        \n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "\n",
    "\n",
    "        x^s_1 & x^s_2 & \\cdots & x^s_{d/h}  \\\\\n",
    "\n",
    "    \\end{bmatrix} \\\\\n",
    "\n",
    "    \\vdots \\\\\n",
    "\n",
    "    \n",
    "    \\begin{bmatrix} \n",
    "\n",
    "        x^1_{d\\frac{(h-1)}{h}+1} & x^1_2 & \\cdots & x^1_d  \\\\\n",
    "\n",
    "        \n",
    "    \\vdots & \\vdots & \\ddots  & \\vdots \\\\\n",
    "\n",
    "\n",
    "        x^s_{d\\frac{(h-1)}{h}+1} & x^s_2 & \\cdots & x^s_d  \\\\\n",
    "\n",
    "    \\end{bmatrix} \\\\\n",
    "\n",
    "\\end{bmatrix} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import Tensor\n",
    "from torch.nn import Module\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def split(sequence: Tensor, number_of_heads: int) -> Tensor:\n",
    "    batch_size, sequence_length, model_dimension = sequence.size()\n",
    "    sequence = sequence.view(batch_size, sequence_length, model_dimension // number_of_heads, number_of_heads)\n",
    "    sequence = sequence.transpose(1, 2).contiguous()\n",
    "    return sequence\n",
    "\n",
    "def concat(sequence: Tensor) -> Tensor:\n",
    "    batch_size, sequence_lenght, heads_dimension, number_of_heads = sequence.size()\n",
    "    sequence = sequence.transpose(1, 2).contiguous()\n",
    "    sequence = sequence.view(batch_size, sequence_lenght, heads_dimension* number_of_heads)\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:        \n",
    "        variance = math.sqrt(key.size(-1))\n",
    "        score = query @ key.transpose(-2, -1) / variance\n",
    "        return softmax(score) @ value\n",
    "    \n",
    "\n",
    "class MultiHead(Module):\n",
    "    def __init__(self, model_dimension: int, key_dimension: int, value_dimension):\n",
    "        super().__init__()\n",
    "        self.query_projection = Linear(key_dimension, model_dimension)\n",
    "        self.key_projection = Linear(key_dimension, model_dimension)\n",
    "        self.value_projection = Linear(value_dimension, model_dimension)\n",
    "\n",
    "    def attention(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:        \n",
    "        variance = math.sqrt(key.size(-1))\n",
    "        score = query @ key.transpose(-2, -1) / variance\n",
    "        return softmax(score) @ value\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        query, key, value = self.query_projection(query), self.key_projection(key), self.value_projection(value)\n",
    "        query, key, value = split(query), split(key), split(value)\n",
    "        attention = self.attention(query, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn.functional import scaled_dot_product_attention"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
